# ğŸ“† instagram_crawler.py - 1000ê°œ ê²Œì‹œë¬¼ë§Œ, ë³¸ë¬¸Â·ì´ë¯¸ì§€Â·ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
# (ë„‰ë„‰í•œ ëœë¤ sleep + ìë™í™” íƒì§€ ìš°íšŒ ê°•í™”)

import os
import time
import re
import pickle
import csv
import random
import traceback
import sys

from pathlib import Path

# Use explicit project directory for cookie storage
PROJECT_DIR = Path("/Users/hb/Desktop/PROJECT")
COOKIE_PATH = os.getenv("COOKIE_PATH", PROJECT_DIR / "cookies.pkl")

from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchWindowException, NoSuchElementException
import requests

def load_instagram_cookies(driver, path=COOKIE_PATH):
    if os.path.exists(path):
        # Ensure on base domain before adding cookies
        driver.get("https://www.instagram.com")
        rand_sleep(2, 4)
        try:
            with open(path, "rb") as f:
                cookies = pickle.load(f)
        except Exception as e:
            print(f"âš ï¸ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        for ck in cookies:
            safe = {
                "name": ck.get("name"), "value": ck.get("value"),
                "domain":   ck["domain"],
                "path": ck.get("path", "/"), "expiry": ck.get("expiry"),
                "secure": ck.get("secure", False),
                "httpOnly": ck.get("httpOnly", False),
                "sameSite": ck.get("sameSite")
            }
            try:
                driver.add_cookie(safe)
            except:
                pass
        print("âœ… ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ")
        driver.refresh()
        rand_sleep(5, 8)
        return True
    return False

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
INSTAGRAM_ID       = os.getenv("INSTAGRAM_ID")
INSTAGRAM_PASSWORD = os.getenv("INSTAGRAM_PASSWORD")
USER_AGENT         = os.getenv(
    "USER_AGENT",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
)
# ì—¬ëŸ¬ ê³„ì • ì§€ì • ì‹œ .env: INSTAGRAM_ACCOUNTS=hongdaeff,another_user
ACCOUNTS = [acct.strip() for acct in os.getenv("INSTAGRAM_ACCOUNTS", "ccyc.live").split(",")]

CHROMEDRIVER_PATH = "/opt/homebrew/bin/chromedriver"
BASE_IMG_DIR      = "images"
MAX_POSTS         = 500
SCROLL_PAUSE_MIN  = 6.0
SCROLL_PAUSE_MAX  = 9.0

# ğŸ” ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
default_patterns = {
    "date":  [r"(\d{4}[./-]\d{2}[./-]\d{2})", r"\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼"],
    "time":  [r"\d{1,2}[:ì‹œ]\d{2}", r"\d{1,2}ì‹œ"],
    "price": [r"([\d,]+)(?=\s*(?:ì›|[Ww]|KRW))"],
    "place": [r"ğŸ“\s*(.+)", r"(ì„œìš¸.*êµ¬.*)"]
}
account_patterns = {
    "hongdaeff": {
        "date":  [r"(\d{4}/\d{2}/\d{2})", r"(\d{1,2}/\d{1,2})"],
        "time":  [r"(\d{1,2}[:.]\d{2})", r"(\d{1,2}PM|\d{1,2}AM)"],
        "price": [
            r"([\d,]+)(?=\s*(?:ì›|[Ww]|KRW))",
            r"(Cover\s*[:\-]?\s*\d{1,3},?\d{3})",
            r"(â‚©\d{1,3},?\d{3})"
        ],
        "place": [r"(ì„œìš¸ì‹œ\s?[^\n\r]+)", r"@hongdaeff"]
    }
}

def convert_time(s):
    if not s:
        return ""
    s = s.strip()
    # 'YYYY.M.D HH:MM:SS AM/PM'
    m_ts = re.match(r'^\d{4}\.\d{1,2}\.\d{1,2}\s+(\d{1,2}):(\d{2}):\d{2}\s*(AM|PM)$', s, re.IGNORECASE)
    if m_ts:
        h, mi, ampm = int(m_ts.group(1)), int(m_ts.group(2)), m_ts.group(3).upper()
        if ampm == 'PM' and h != 12: h += 12
        if ampm == 'AM' and h == 12: h = 0
        return f"{h:02d}:{mi:02d}"
    # 'HH:MM'
    m = re.match(r'^(\d{1,2}):(\d{2})$', s)
    if m:
        return f"{int(m.group(1)):02d}:{int(m.group(2)):02d}"
    # 'Xì‹œ Yë¶„' or 'Xì‹œYë¶„'
    m_kr = re.match(r'^(\d{1,2})ì‹œ\s*(\d{1,2})ë¶„$', s)
    if m_kr:
        return f"{int(m_kr.group(1)):02d}:{int(m_kr.group(2)):02d}"
    # 'Xì‹œMM'
    m4 = re.match(r'^(\d{1,2})ì‹œ\s*(\d{1,2})$', s)
    if m4:
        return f"{int(m4.group(1)):02d}:{int(m4.group(2)):02d}"
    # 'Xì‹œ'
    m3 = re.match(r'^(\d{1,2})ì‹œ$', s)
    if m3:
        return f"{int(m3.group(1)):02d}:00"
    return ""

def multi_search(patterns, text):
    for pat in patterns:
        m = re.search(pat, text)
        if m:
            return (m.group(1) if m.lastindex else m.group(0)).strip()
    return ""

def rand_sleep(a, b):
    time.sleep(random.uniform(a, b))

def smooth_scroll(driver):
    driver.execute_script("window.scrollBy(0, 400);")
    rand_sleep(SCROLL_PAUSE_MIN, SCROLL_PAUSE_MAX)

# ğŸš€ WebDriver ì‹œì‘ ë° ìë™í™” íƒì§€ ìš°íšŒ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument(f"--user-agent={USER_AGENT}")
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option("useAutomationExtension", False)
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

service = Service(CHROMEDRIVER_PATH)
driver  = webdriver.Chrome(service=service, options=options)
wait    = WebDriverWait(driver, 25)

# navigator.webdriver ì†ì„± ê°ì¶”ê¸°
driver.execute_cdp_cmd(
    "Page.addScriptToEvaluateOnNewDocument",
    {"source": """
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
    """}
)


# ğŸ” ì¿ í‚¤ ë¡œê·¸ì¸ ìš°ì„ 
driver.get("https://www.instagram.com")
rand_sleep(8, 12)
if not load_instagram_cookies(driver):
    print("ğŸ” ì¿ í‚¤ ì—†ìŒ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ. ìˆ˜ë™ ë¡œê·¸ì¸ í›„ Enter:")
    input()
    with open(COOKIE_PATH, "wb") as f:
        pickle.dump(driver.get_cookies(), f)
    print("âœ… ì¿ í‚¤ ì €ì¥ ì™„ë£Œ")
rand_sleep(8, 12)

try:
    # ğŸ”„ ê³„ì •ë³„ í¬ë¡¤ë§
    for account in ACCOUNTS:
        csv_file = f"{account}_full.csv"
        existing_links = []
        existing_results = []
        if os.path.exists(csv_file):
            with open(csv_file, newline="", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    existing_links.append(row["link"])
                    existing_results.append(row)
            print(f"ğŸ”„ ê¸°ì¡´ì— ì €ì¥ëœ ê²Œì‹œë¬¼ {len(existing_links)}ê°œ ë¡œë“œ")
        else:
            existing_links = []
            existing_results = []

        print(f"\n=== Crawling @{account} ===")
        img_dir = os.path.join(BASE_IMG_DIR, account)
        os.makedirs(img_dir, exist_ok=True)

        # â–¶ í”„ë¡œí•„ í˜ì´ì§€ ë¡œë“œ & ëŒ€ê¸°
        driver.get(f"https://www.instagram.com/{account}/")
        rand_sleep(7, 10)

        # â–¶ ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘ (ìµœëŒ€ 1000ê°œ)
        all_links = []
        prev_count = 0
        attempts = 0
        while len(all_links) < MAX_POSTS:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            rand_sleep(SCROLL_PAUSE_MIN, SCROLL_PAUSE_MAX)

            anchors = driver.find_elements(By.CSS_SELECTOR, "a[href*='/p/']")
            for a in anchors:
                href = a.get_attribute("href")
                if href and href not in all_links:
                    all_links.append(href)
                    if len(all_links) >= MAX_POSTS:
                        break

            new_count = len(all_links)
            if new_count == prev_count:
                attempts += 1
                if attempts >= 3:
                    break
            else:
                attempts = 0
                prev_count = new_count
            print(f"ğŸ”„ ìŠ¤í¬ë¡¤ ì§„í–‰ ì¤‘: {len(all_links)}/{MAX_POSTS} ë§í¬ ìˆ˜ì§‘")

        print(f"ğŸ” ì´ {len(all_links)}ê°œ ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘")

        if existing_links:
            if existing_links[-1] in all_links:
                last_index = all_links.index(existing_links[-1])
                new_links = all_links[last_index + 1:]
            else:
                new_links = all_links
        else:
            new_links = all_links
        print(f"ğŸ” ìƒˆë¡œ í¬ë¡¤ë§í•  ë§í¬ {len(new_links)}ê°œ")

        # â–¶ ê²Œì‹œë¬¼ ì—´ì–´ì„œ ì •ë³´ ì¶”ì¶œ
        results = existing_results.copy()
        start_idx = len(existing_results) + 1
        for offset, link in enumerate(new_links):
            idx = start_idx + offset
            print(f"  {idx}. Opening: {link}")
            try:
                driver.get(link)
                rand_sleep(7, 10)

                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article header")))
                rand_sleep(7, 10)

                try:
                    content = driver.find_element(By.CSS_SELECTOR, "article header h1").text
                except NoSuchElementException:
                    try:
                        content = driver.find_element(By.TAG_NAME, "h1").text
                    except NoSuchElementException:
                        content = driver.find_element(By.TAG_NAME, "span").text

                # 3) ì •ê·œí‘œí˜„ì‹ ì •ë³´ ì¶”ì¶œ
                patterns   = account_patterns.get(account, default_patterns)
                place      = multi_search(patterns["place"],  content)
                date       = multi_search(patterns["date"],   content)
                raw_time  = multi_search(patterns["time"],   content)
                time_info = convert_time(raw_time)
                # ê°€ê²© ì •ë³´ ì¶”ì¶œ ì´ˆê¸°í™”
                in_adv = ""
                cover = ""
                # í†µí•© ì •ê·œí‘œí˜„ì‹: ì‚¬ì „ì˜ˆë§¤/í˜„ì¥ì˜ˆë§¤ êµ¬ë¶„ ë° ë‹¨ìœ„(ë§Œì›/ì›) ì²˜ë¦¬
                price_pattern = re.compile(
                    r"(ì‚¬ì „ì˜ˆë§¤)\s*([\d\.]+)ë§Œì›"
                    r"|(?:í˜„ì¥ì˜ˆë§¤|í˜„ë§¤)\s*([\d\.]+)ë§Œì›"
                    r"|([\d\.]+)ë§Œì›"
                    r"|([\d,]+)(?=\s*(?:ì›|[Ww]|KRW))",
                    flags=re.IGNORECASE
                )
                matches = price_pattern.findall(content)
                for grp1, grp2, grp3, grp4, grp5, grp6 in matches:
                    if grp1:  # ì‚¬ì „ì˜ˆë§¤ xë§Œì›
                        value = float(grp2)
                        in_adv = str(int(value * 10000))
                    elif grp3:  # í˜„ì¥ì˜ˆë§¤/í˜„ë§¤ xë§Œì›
                        value = float(grp4)
                        cover = str(int(value * 10000))
                    elif grp3 == "" and grp1 == "" and grp5:  # ì ‘ë‘ì–´ ì—†ëŠ” xë§Œì›
                        value = float(grp5)
                        if not cover:
                            cover = str(int(value * 10000))
                    elif grp6:  # ì› ë‹¨ìœ„ (comma í¬í•¨)
                        raw = grp6.replace(",", "")
                        if not in_adv:
                            in_adv = raw
                        elif not cover:
                            cover = raw
                # 'ë¬´ë£Œ'ê°€ í¬í•¨ëœ ê²½ìš° coverë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                if "ë¬´ë£Œ" in content:
                    cover = "0"

                # 4) ê²°ê³¼ ì €ì¥
                results.append({
                    "link":    link,
                    "content": content,
                    "place":   place,
                    "date":    date,
                    "time":    time_info,
                    "in advance": in_adv,
                    "cover":      cover
                })

            except TimeoutException:
                print("    âš ï¸ ìš”ì†Œ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            except NoSuchWindowException:
                print("âš ï¸ ë¸Œë¼ìš°ì €ê°€ ë‹«í˜”ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    â— ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\n{traceback.format_exc()}")
            finally:
                rand_sleep(7, 10)

        # ğŸ’¾ CSV ì €ì¥
        with open(csv_file, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["link","content","place","date","time","in advance","cover"]
            )
            writer.writeheader()
            writer.writerows(results)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {csv_file}")

except KeyboardInterrupt:
    print("\nğŸ”” í¬ë¡¤ë§ ì¤‘ë‹¨ ì‹ í˜¸ ê°ì§€: í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
    # íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ë©ˆì¶˜ ê³„ì •ì˜ CSV íŒŒì¼ì— ë¶€ë¶„ ì €ì¥
    with open(csv_file, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["link","content","place","date","time","in advance","cover"]
        )
        writer.writeheader()
        writer.writerows(results)
    print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {csv_file}")
    driver.quit()
    sys.exit(0)

except Exception as e:
    print("\nâ— ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ! í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
    with open(csv_file, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["link","content","place","date","time","in advance","cover"]
        )
        writer.writeheader()
        writer.writerows(results)
    print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {csv_file}")
    driver.quit()
    sys.exit(1)

# ğŸ›‘ ì¢…ë£Œ
driver.quit()